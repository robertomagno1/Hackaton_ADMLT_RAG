{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":100993,"databundleVersionId":12484716,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:46:18.486550Z","iopub.execute_input":"2025-05-28T11:46:18.486750Z","iopub.status.idle":"2025-05-28T11:46:21.477750Z","shell.execute_reply.started":"2025-05-28T11:46:18.486732Z","shell.execute_reply":"2025-05-28T11:46:21.476994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\n\n# Load datasets\ncorpus_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/corpus.csv\")\ntrain_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/test.csv\")\nexample_submission_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/example_submission.csv\")\n\n# Preview datasets\nprint(\"Corpus:\", corpus_df.shape)\nprint(corpus_df.head(2), \"\\n\")\n\nprint(\"Train:\", train_df.shape)\nprint(train_df.head(2), \"\\n\")\n\nprint(\"Test:\", test_df.shape)\nprint(test_df.head(2), \"\\n\")\n\nprint(\"Submission Format:\")\nprint(example_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:46:21.479229Z","iopub.execute_input":"2025-05-28T11:46:21.479537Z","iopub.status.idle":"2025-05-28T11:46:23.283710Z","shell.execute_reply.started":"2025-05-28T11:46:21.479517Z","shell.execute_reply":"2025-05-28T11:46:23.283047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 1: Retriever (Dense Embeddings + FAISS)\n","metadata":{}},{"cell_type":"markdown","source":"http://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\n\nsentence-transformers model based on BioBERT, trained on datasets relevant for natural language inference, semantic similarity, and biomedical QA\n\nMNLI, SNLI: Natural Language Inference\n\nSCINLI, SCITAIL, MEDNLI: Science and Medical entailment datasets\n\nSTSB: Sentence-level similarity benchmark\n\nIt outputs 768-dimensional embeddings that capture sentence meaning, ideal for semantic search in biomedical texts.\n\n","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# Step 1: Load biomedical passages\n# ---------------------------\n\ncorpus_texts = corpus_df[\"passage\"].tolist() # actual biomedical text\ncorpus_ids = corpus_df[\"id\"].tolist() # unique IDs (idk if useful)\n# our retrievable \"knowledge\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:46:31.803137Z","iopub.execute_input":"2025-05-28T11:46:31.803396Z","iopub.status.idle":"2025-05-28T11:46:31.811277Z","shell.execute_reply.started":"2025-05-28T11:46:31.803375Z","shell.execute_reply":"2025-05-28T11:46:31.810548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \\\n  transformers sentence-transformers faiss-cpu\n\"\"\"\n# Run once per session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:46:34.946961Z","iopub.execute_input":"2025-05-28T11:46:34.947212Z","iopub.status.idle":"2025-05-28T11:48:03.593453Z","shell.execute_reply.started":"2025-05-28T11:46:34.947194Z","shell.execute_reply":"2025-05-28T11:48:03.592522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  \n# We use GPU to improve performance (timely speaking)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:48:54.962830Z","iopub.execute_input":"2025-05-28T11:48:54.963404Z","iopub.status.idle":"2025-05-28T11:48:54.968224Z","shell.execute_reply.started":"2025-05-28T11:48:54.963381Z","shell.execute_reply":"2025-05-28T11:48:54.967527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# ---------------------------\n# Step 2: Load BioBERT model\n# ---------------------------\n\nmodel_name = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb' # It maps each sentence to a dense vector\ntokenizer = AutoTokenizer.from_pretrained(model_name) # convert sentences into token IDs\nmodel = AutoModel.from_pretrained(model_name) # BioBERT model fine-tuned for semantic similarity tasks\nmodel.eval() # turns off dropout etc. for inference\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n# Get token-level embeddings (This is BioBERT outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:48:10.183685Z","iopub.execute_input":"2025-05-28T11:48:10.184602Z","iopub.status.idle":"2025-05-28T11:48:54.961478Z","shell.execute_reply.started":"2025-05-28T11:48:10.184560Z","shell.execute_reply":"2025-05-28T11:48:54.960268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport faiss\n\n# ---------------------------\n# Step 3: Mean Pooling Function\n# ---------------------------\n\ndef mean_pooling(model_output, attention_mask):\n    \"\"\"\n    Aggregate token embeddings into one 768-dim vector per sentence\n    \"\"\"\n    token_embeddings = model_output[0]  # (batch_size, seq_len, hidden_size)\n    # Multiplies each token vector by its attention mask (to ignore padding).\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    # Averages valid tokens to get one sentence embedding \n    return torch.sum(token_embeddings * input_mask_expanded, 1) / \\\n           torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# ---------------------------\n# Step 4: Encode Corpus Passages\n# ---------------------------# \n\nbatch_size = 32\ncorpus_embeddings = []\n\n# loop over the corpus in batches of 32 passages to avoid GPU memory overflow\nfor i in tqdm(range(0, len(corpus_texts), batch_size)):\n    batch_texts = corpus_texts[i:i+batch_size]\n    # tokenize a batch\n    encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=256)\n    # move the tokenize batch to GPU\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    # pass input through BioBERT\n    with torch.no_grad():\n        model_output = model(**encoded_input) # disable gradient computation for speed\n    # pool token embeddings to sentence vectors\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n    # collect them into a list\n    corpus_embeddings.append(sentence_embeddings.cpu().numpy())\n    \n# stack all batches into a single 2D NumPy array: shape (40181, 768)\ncorpus_embeddings = np.vstack(corpus_embeddings)\n\n# Normalizes each embedding to unit length (L2 norm = 1), so inner product = cosine similarity\nfaiss.normalize_L2(corpus_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:52:00.187692Z","iopub.execute_input":"2025-05-28T11:52:00.188446Z","iopub.status.idle":"2025-05-28T12:01:34.703101Z","shell.execute_reply.started":"2025-05-28T11:52:00.188412Z","shell.execute_reply":"2025-05-28T12:01:34.702089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\n# Normalizes each embedding to unit length (L2 norm = 1), so inner product = cosine similarity\nfaiss.normalize_L2(corpus_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:02:24.292093Z","iopub.execute_input":"2025-05-28T12:02:24.292813Z","iopub.status.idle":"2025-05-28T12:02:24.304620Z","shell.execute_reply.started":"2025-05-28T12:02:24.292788Z","shell.execute_reply":"2025-05-28T12:02:24.303866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Step 5: Create FAISS Index\n# ---------------------------\n\nindex = faiss.IndexFlatIP(corpus_embeddings.shape[1]) # IndexFlatIP: Flat (non-hierarchical) index using Inner Product (dot product)\n# Thanks to normalization, dot product ≈ cosine similarity\nindex.add(corpus_embeddings) # dadd all embeddings to the index\nprint(f\"FAISS index created with {index.ntotal} vectors.\")\n# Efficient similarity search","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:02:26.208043Z","iopub.execute_input":"2025-05-28T12:02:26.208315Z","iopub.status.idle":"2025-05-28T12:02:26.327847Z","shell.execute_reply.started":"2025-05-28T12:02:26.208295Z","shell.execute_reply":"2025-05-28T12:02:26.327076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Step 6: Define Retrieval Function\n# ---------------------------\n\ndef retrieve_top_k(question, k=5):\n    \"\"\"\n    These passages are used for answering the question\n    \"\"\"\n    # Tokenizes one question\n    encoded = tokenizer([question], return_tensors='pt', padding=True, truncation=True, max_length=256)\n    # and send it ot GPU\n    encoded = {k: v.to(device) for k, v in encoded.items()}\n\n    with torch.no_grad():\n        output = model(**encoded)\n    # Gets the sentence embedding for the question\n    q_embed = mean_pooling(output, encoded['attention_mask'])\n    # Converts to NumPy and normalizes so it’s comparable with FAISS index\n    q_embed = q_embed.cpu().numpy()\n    faiss.normalize_L2(q_embed)\n    # FAISS retrieves top-k most similar vectors from the index\n    scores, indices = index.search(q_embed, k)\n    top_passages = [corpus_texts[i] for i in indices[0]]\n    return top_passages\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:02:30.252035Z","iopub.execute_input":"2025-05-28T12:02:30.252305Z","iopub.status.idle":"2025-05-28T12:02:30.258483Z","shell.execute_reply.started":"2025-05-28T12:02:30.252284Z","shell.execute_reply":"2025-05-28T12:02:30.257718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"https://huggingface.co/ktrapeznikov/biobert_v1.1_pubmed_squad_v2\n\nBioBERT model fine-tuned on SQuAD v2, designed for extractive QA in biomedical domains\n\npretrained on PubMed abstracts\n\nFine-tuned on SQuAD v2 — includes unanswerable questions logic\n\nF1 ~79 on SQuAD v2, suitable for biomedical-style QA","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\n\n# ---------------------------\n# Step 7: QA model \n# ---------------------------\n\nqa_model_name = \"ktrapeznikov/biobert_v1.1_pubmed_squad_v2\"\nqa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\nqa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)\nqa_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:06:29.223082Z","iopub.execute_input":"2025-05-28T12:06:29.223588Z","iopub.status.idle":"2025-05-28T12:06:33.444911Z","shell.execute_reply.started":"2025-05-28T12:06:29.223562Z","shell.execute_reply":"2025-05-28T12:06:33.444171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Step 8: Generate Answer from Retrieved Passages\n# ---------------------------\n\ndef generate_answer(question, contexts):\n    best_answer = \"\"\n    best_score = -float(\"inf\")\n    # Loop through the top-k retrieved contexts\n    for context in contexts:\n        # Prepares input for QA model as question-context pair\n        inputs = qa_tokenizer( \n            question,\n            context,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=True,\n            max_length=512,\n        ).to(device)\n\n        with torch.no_grad():\n            outputs = qa_model(**inputs) # Predicts scores for all possible start and end positions\n\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n        # Get the most likely  start/end positions\n        start_idx = torch.argmax(start_logits) \n        end_idx = torch.argmax(end_logits)\n\n        # Validate answer span\n        if start_idx <= end_idx:\n            answer_tokens = inputs[\"input_ids\"][0][start_idx : end_idx + 1]\n            answer = qa_tokenizer.decode(answer_tokens, skip_special_tokens=True) # Converts the token IDs to an actual string\n            score = start_logits[0, start_idx] + end_logits[0, end_idx] # sums logits (confidence) for the answer span\n\n            # Keeps only the best answer found so far\n            if score > best_score and answer.strip():\n                best_answer = answer.strip()\n                best_score = score\n\n    return best_answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:12:25.367385Z","iopub.execute_input":"2025-05-28T12:12:25.367649Z","iopub.status.idle":"2025-05-28T12:12:25.373465Z","shell.execute_reply.started":"2025-05-28T12:12:25.367628Z","shell.execute_reply":"2025-05-28T12:12:25.372630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Step 9: Run on Test Set\n# ---------------------------\n\npredictions = []\n\n# Iterates through every test question\nfor question in tqdm(test_df[\"question\"]):\n    retrieved_contexts = retrieve_top_k(question, k=5) # Retrieves the top-5 most similar passages using FAISS + BioBERT embeddings\n    answer = generate_answer(question, retrieved_contexts) # Uses the QA model (BioBERT-SQuAD) to extract the most likely answer from those passages\n    predictions.append(answer)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:12:44.352412Z","iopub.execute_input":"2025-05-28T12:12:44.352652Z","iopub.status.idle":"2025-05-28T12:15:17.478386Z","shell.execute_reply.started":"2025-05-28T12:12:44.352636Z","shell.execute_reply":"2025-05-28T12:15:17.477617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Step 10: Generate submission\n# ---------------------------\n\nexample_submission_df[\"answer\"] = predictions\nexample_submission_df.to_csv(\"submission2.csv\", index=False)\n\nprint(example_submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:16:11.557253Z","iopub.execute_input":"2025-05-28T12:16:11.557838Z","iopub.status.idle":"2025-05-28T12:16:11.568514Z","shell.execute_reply.started":"2025-05-28T12:16:11.557790Z","shell.execute_reply":"2025-05-28T12:16:11.567776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for q, a in zip(test_df[\"question\"].head(10), predictions[:10]):\n    print(f\"\\nQuestion: {q}\\nAnswer: {a}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:16:15.587330Z","iopub.execute_input":"2025-05-28T12:16:15.587806Z","iopub.status.idle":"2025-05-28T12:16:15.592271Z","shell.execute_reply.started":"2025-05-28T12:16:15.587782Z","shell.execute_reply":"2025-05-28T12:16:15.591577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing = 0\ntotal = 0\nf1_total = 0\n\nfor q, a in zip(test_df[\"question\"], predictions):\n    \n    total += 1\n    if not a.strip():  # risposta vuota\n        missing += 1\n        f1_total += 0.0\n    else:  # risposta presente, assumiamo perfetta\n        f1_total += 1.0\n\n# Riassunto\nprint(\"\\n--- STATISTICHE ---\")\nprint(f\"Totale domande valutate: {total}\")\nprint(f\"Risposte mancanti: {missing}\")\nprint(f\"Risposte presenti: {total - missing}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:32:53.037501Z","iopub.execute_input":"2025-05-28T12:32:53.037784Z","iopub.status.idle":"2025-05-28T12:32:53.043739Z","shell.execute_reply.started":"2025-05-28T12:32:53.037764Z","shell.execute_reply":"2025-05-28T12:32:53.043107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By default the model we used returns empty answer when the score of its response is under a certain threshold (so he is not sure about it). The issue was that the Kaggle submission set up didn't allowed the used of any empty space so replaced the empty answer with a placeholder, to allow us to submit our predictions","metadata":{}},{"cell_type":"code","source":"# Fill in missing answers with a default string\nexample_submission_df[\"answer\"] = example_submission_df[\"answer\"].fillna(\"No\")\nexample_submission_df.loc[example_submission_df[\"answer\"].str.strip() == \"\", \"answer\"] = \"No\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_path = \"/kaggle/working/submission_na.csv\"\nexample_submission_df.to_csv(submission_path, index=False)\n\nprint(\"Submission file created:\", submission_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}