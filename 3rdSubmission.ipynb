{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":100993,"databundleVersionId":12484716,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:29:19.380246Z","iopub.execute_input":"2025-05-30T14:29:19.380496Z","iopub.status.idle":"2025-05-30T14:29:21.257303Z","shell.execute_reply.started":"2025-05-30T14:29:19.380478Z","shell.execute_reply":"2025-05-30T14:29:21.256630Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/adm-lt-2024-2025-hackathon-rag/corpus.csv\n/kaggle/input/adm-lt-2024-2025-hackathon-rag/example_submission.csv\n/kaggle/input/adm-lt-2024-2025-hackathon-rag/train.csv\n/kaggle/input/adm-lt-2024-2025-hackathon-rag/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\n\n# Load datasets\ncorpus_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/corpus.csv\")\ntrain_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/test.csv\")\nexample_submission_df = pd.read_csv(\"/kaggle/input/adm-lt-2024-2025-hackathon-rag/example_submission.csv\")\n\n# Preview datasets\nprint(\"Corpus:\", corpus_df.shape)\nprint(corpus_df.head(2), \"\\n\")\n\nprint(\"Train:\", train_df.shape)\nprint(train_df.head(2), \"\\n\")\n\nprint(\"Test:\", test_df.shape)\nprint(test_df.head(2), \"\\n\")\n\nprint(\"Submission Format:\")\nprint(example_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:29:21.258124Z","iopub.execute_input":"2025-05-30T14:29:21.258410Z","iopub.status.idle":"2025-05-30T14:29:23.073844Z","shell.execute_reply.started":"2025-05-30T14:29:21.258384Z","shell.execute_reply":"2025-05-30T14:29:23.073096Z"}},"outputs":[{"name":"stdout","text":"Corpus: (40181, 2)\n                                             passage     id\n0  New data on viruses isolated from patients wit...   9797\n1  We describe an improved method for detecting d...  11906 \n\nTrain: (3775, 4)\n                                            question  \\\n0  Name the algorithms for counting multi-mapping...   \n1  Which R/bioconductor package is used for integ...   \n\n                                              answer    id  \\\n0  RNA-Seq is currently used routinely, and it pr...  3009   \n1  Sushi.R is a flexible, quantitative and integr...   331   \n\n  relevant_passage_ids  \n0  [29444201 28915787]  \n1           [24903420]   \n\nTest: (1180, 2)\n     id                                  question\n0  1702  PBT2 has been tested for which disorder?\n1  3135          What does Prevnar 13 consist of? \n\nSubmission Format:\n     id             answer\n0  1702  this is an answer\n1  3135  this is an answer\n2   650  this is an answer\n3   741  this is an answer\n4  2949  this is an answer\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Stage 1: Retriever (Dense Embeddings + FAISS)\n","metadata":{}},{"cell_type":"markdown","source":"http://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\n\nsentence-transformers model based on BioBERT, trained on datasets relevant for natural language inference, semantic similarity, and biomedical QA\n\nMNLI, SNLI: Natural Language Inference\n\nSCINLI, SCITAIL, MEDNLI: Science and Medical entailment datasets\n\nSTSB: Sentence-level similarity benchmark\n\nIt outputs 768-dimensional embeddings that capture sentence meaning, ideal for semantic search in biomedical texts.\n\n","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# Step 1: Load biomedical passages\n# ---------------------------\n\ncorpus_texts = corpus_df[\"passage\"].tolist() # actual biomedical text\ncorpus_ids = corpus_df[\"id\"].tolist() # unique IDs (idk if useful)\n# our retrievable \"knowledge\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:29:23.075547Z","iopub.execute_input":"2025-05-30T14:29:23.075759Z","iopub.status.idle":"2025-05-30T14:29:23.084197Z","shell.execute_reply.started":"2025-05-30T14:29:23.075742Z","shell.execute_reply":"2025-05-30T14:29:23.083347Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \\\n  transformers sentence-transformers faiss-cpu\n\n# Run once per session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:32:14.212074Z","iopub.execute_input":"2025-05-30T14:32:14.212850Z","iopub.status.idle":"2025-05-30T14:33:29.935588Z","shell.execute_reply.started":"2025-05-30T14:32:14.212821Z","shell.execute_reply":"2025-05-30T14:33:29.934649Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  \n# We use GPU to improve performance (timely speaking)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:29:23.107871Z","iopub.execute_input":"2025-05-30T14:29:23.108080Z","iopub.status.idle":"2025-05-30T14:29:27.728125Z","shell.execute_reply.started":"2025-05-30T14:29:23.108064Z","shell.execute_reply":"2025-05-30T14:29:27.727352Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# ---------------------------\n# Step 2: Load BioBERT model\n# ---------------------------\n\nmodel_name = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb' # It maps each sentence to a dense vector\ntokenizer = AutoTokenizer.from_pretrained(model_name) # convert sentences into token IDs\nmodel = AutoModel.from_pretrained(model_name) # BioBERT model fine-tuned for semantic similarity tasks\nmodel.eval() # turns off dropout etc. for inference\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n# Get token-level embeddings (This is BioBERT outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:29:27.728915Z","iopub.execute_input":"2025-05-30T14:29:27.729229Z","iopub.status.idle":"2025-05-30T14:29:56.659728Z","shell.execute_reply.started":"2025-05-30T14:29:27.729212Z","shell.execute_reply":"2025-05-30T14:29:56.659191Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bdb93105124064804ec6e92c1173eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031646b6c8994e0db9bc9cb2a3d37a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9584cd41a9834eab841c6a7a9464fc61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def1b88f9fc24bfa919a7aa85eb092c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27451e7f7674a6fabbb987613bfd27b"}},"metadata":{}},{"name":"stderr","text":"2025-05-30 14:29:42.519070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748615382.722386      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748615382.781213      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ba88ee3a75409fa15e454de980cca2"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import gc\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom sentence_transformers import SentenceTransformer\n\n# === SETTINGS ===\nEMBEDDING_MODEL_NAME = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\nCHUNK_SIZE = 512\nCHUNK_OVERLAP = 50\n\n# === CLEAN GPU MEMORY ===\ndef clear_cuda():\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\nclear_cuda()\n\n# === LOAD TOKENIZER & MODEL ===\ntokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\nmodel = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cuda\")\n\n# === LANGCHAIN SPLITTER ===\nsplitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n    tokenizer=tokenizer,\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n    add_start_index=True,\n    strip_whitespace=True,\n)\n\n# === CONVERT corpus_df TO LangchainDocuments ===\ndocuments = []\nfor _, row in tqdm(corpus_df.iterrows(), total=len(corpus_df), desc=\"Preparing docs\"):\n    doc = LangchainDocument(\n        page_content=row[\"passage\"],\n        metadata={\"source\": row[\"id\"]}  # you can extend this if needed\n    )\n    documents.append(doc)\n\n# === SPLIT INTO CHUNKS ===\ndocs_processed = []\nfor doc in tqdm(documents, desc=\"Splitting into chunks\"):\n    docs_processed.extend(splitter.split_documents([doc]))\n\n# === REMOVE DUPLICATES ===\nunique_docs = list({doc.page_content: doc for doc in docs_processed}.values())\n\n# === CONVERT BACK TO DataFrame ===\nchunked_df = pd.DataFrame([\n    {\n        \"chunk\": doc.page_content,\n        \"source_id\": doc.metadata[\"source\"],\n        \"start_index\": doc.metadata.get(\"start_index\", -1)\n    }\n    for doc in unique_docs\n])\n\nprint(\"Chunking complete.\")\nprint(\"Total unique chunks:\", len(chunked_df))\nprint(chunked_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:51:10.845915Z","iopub.execute_input":"2025-05-30T14:51:10.846560Z","iopub.status.idle":"2025-05-30T14:54:20.027176Z","shell.execute_reply.started":"2025-05-30T14:51:10.846535Z","shell.execute_reply":"2025-05-30T14:54:20.026316Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be4e7e34db7542e98f32b2769ef26381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2207b5a23a47eaaab8d69adbaf75af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbcbc1527f704252a0588aceca7e7b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac65d87f6654f3d89e48c3be0c5bab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7baa81dfeea94c6a913e2d17d3246f28"}},"metadata":{}},{"name":"stderr","text":"Preparing docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40181/40181 [00:01<00:00, 22972.36it/s]\nSplitting into chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40181/40181 [03:00<00:00, 222.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Chunking complete.\nTotal unique chunks: 52339\n                                               chunk  source_id  start_index\n0  New data on viruses isolated from patients wit...       9797            0\n1  We describe an improved method for detecting d...      11906            0\n2  We have studied the effects of curare on respo...      16083            0\n3  Kinetic and electrophoretic properties of 230-...      23188            0\n4  Male Wistar specific-pathogen-free rats aged 2...      23469            0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:55:10.495099Z","iopub.execute_input":"2025-05-30T14:55:10.495342Z","iopub.status.idle":"2025-05-30T14:55:20.184222Z","shell.execute_reply.started":"2025-05-30T14:55:10.495325Z","shell.execute_reply":"2025-05-30T14:55:20.183420Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain-core<1.0.0,>=0.3.59 (from langchain_community)\n  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain<1.0.0,>=0.3.25 (from langchain_community)\n  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.18)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain_community)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.59->langchain_community)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, packaging, httpx-sse, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.50\n    Uninstalling langchain-core-0.3.50:\n      Successfully uninstalled langchain-core-0.3.50\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.7\n    Uninstalling langchain-text-splitters-0.3.7:\n      Successfully uninstalled langchain-text-splitters-0.3.7\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.22\n    Uninstalling langchain-0.3.22:\n      Successfully uninstalled langchain-0.3.22\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed httpx-sse-0.4.0 langchain-0.3.25 langchain-core-0.3.63 langchain-text-splitters-0.3.8 langchain_community-0.3.24 packaging-24.2 pydantic-settings-2.9.1 python-dotenv-1.1.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Inizializza modello BioBERT per embedding\nembedding_model = HuggingFaceEmbeddings(\n    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # cosine similarity\n)\n\n# Estrai solo i testi dai Documenti\ntexts = [doc.page_content for doc in docs_processed]\n\n# Calcolo embeddings con barra di progresso\nprint(\" Calcolo degli embeddings per ogni documento...\")\n\nembeddings = []\n\nfor text in tqdm(texts, desc=\"Embedding passages\", ncols=100):\n    embedding = embedding_model.embed_query(text)\n    embeddings.append(embedding)\n\nprint(\"Embeddings completati:\", len(embeddings))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:55:34.738359Z","iopub.execute_input":"2025-05-30T14:55:34.738649Z","iopub.status.idle":"2025-05-30T15:05:55.220116Z","shell.execute_reply.started":"2025-05-30T14:55:34.738623Z","shell.execute_reply":"2025-05-30T15:05:55.219274Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1820959558.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding_model = HuggingFaceEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":" Calcolo degli embeddings per ogni documento...\n","output_type":"stream"},{"name":"stderr","text":"Embedding passages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52345/52345 [10:18<00:00, 84.64it/s]","output_type":"stream"},{"name":"stdout","text":"Embeddings completati: 52345\n Creazione del FAISS index...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1820959558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Crea FAISS index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Creazione del FAISS index...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m vectorstore = FAISS.from_embeddings(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FAISS' is not defined"],"ename":"NameError","evalue":"name 'FAISS' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"markdown","source":"I couldn't upload Faiss for GPU used, since here we have only CPU version that was too slow\n\nApparently the issue is regarding the versions, Python and CUDA doesn't match and in Kaggle you can't even change the environment. Switching into colab we managed to create a colab environment where the installation worked, but then a wrong version of numpy ","metadata":{}},{"cell_type":"code","source":"from cuml.neighbors import NearestNeighbors\nimport cupy as cp\n\n# embeddings is a numpy array, convert in cupy\nX = cp.asarray(embeddings)\n\nnn = NearestNeighbors(n_neighbors=10, metric='cosine')\nnn.fit(X)\n\ndistances, indices = nn.kneighbors(X[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:54:07.145287Z","iopub.execute_input":"2025-05-30T15:54:07.145994Z","iopub.status.idle":"2025-05-30T15:54:22.790517Z","shell.execute_reply.started":"2025-05-30T15:54:07.145971Z","shell.execute_reply":"2025-05-30T15:54:22.789896Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"indices = indices.get()  # convert from cupy to numpy\n# Get top-k passages for the first item in X[:5]\ntop_k_texts = [texts[int(i)] for i in indices[0]]\nall_top_k_texts = [[texts[int(i)] for i in row] for row in indices]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:07:48.715486Z","iopub.execute_input":"2025-05-30T16:07:48.715754Z","iopub.status.idle":"2025-05-30T16:07:48.721310Z","shell.execute_reply.started":"2025-05-30T16:07:48.715737Z","shell.execute_reply":"2025-05-30T16:07:48.720611Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"question = \"What disorder was the patient diagnosed with?\"\n\nanswers = []\nfor passage in top_k_texts:\n    input_qa = qa_tokenizer.encode_plus(question, passage, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = qa_model(**input_qa)\n    start = torch.argmax(outputs.start_logits)\n    end = torch.argmax(outputs.end_logits)\n    answer = qa_tokenizer.decode(input_qa[\"input_ids\"][0][start:end+1])\n    answers.append(answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:07:50.045501Z","iopub.execute_input":"2025-05-30T16:07:50.045766Z","iopub.status.idle":"2025-05-30T16:07:50.287452Z","shell.execute_reply.started":"2025-05-30T16:07:50.045746Z","shell.execute_reply":"2025-05-30T16:07:50.286673Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Simple heuristic: longest answer\nfinal_answer = max(answers, key=len)\nprint(\"ğŸ§  Final Answer:\", final_answer)\n# ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:07:53.152002Z","iopub.execute_input":"2025-05-30T16:07:53.152648Z","iopub.status.idle":"2025-05-30T16:07:53.156633Z","shell.execute_reply.started":"2025-05-30T16:07:53.152624Z","shell.execute_reply":"2025-05-30T16:07:53.155992Z"}},"outputs":[{"name":"stdout","text":"ğŸ§  Final Answer: depressive pseudodementia\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# === RETRIEVAL FUNCTION ===\ndef retrieve_top_k(question, k=5):\n    # Ottieni embedding normalizzato per la domanda\n    q_embed = embedding_model.embed_query(question)\n    q_embed_gpu = cp.asarray([q_embed])  # shape: (1, 768)\n\n    # Trova i vicini piÃ¹ vicini tra i documenti indicizzati\n    distances, indices = nn.kneighbors(q_embed_gpu, n_neighbors=k)\n    indices = indices.get()  # da cupy a numpy\n\n    # Ritorna i testi dei documenti top-k\n    top_k_chunks = [texts[int(i)] for i in indices[0]]\n    return top_k_chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:13:18.069573Z","iopub.execute_input":"2025-05-30T16:13:18.069948Z","iopub.status.idle":"2025-05-30T16:13:18.074990Z","shell.execute_reply.started":"2025-05-30T16:13:18.069926Z","shell.execute_reply":"2025-05-30T16:13:18.074172Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nqa_model_name = \"ktrapeznikov/biobert_v1.1_pubmed_squad_v2\"\nqa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\nqa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(\"cuda\")\nqa_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:13:27.930869Z","iopub.execute_input":"2025-05-30T16:13:27.931719Z","iopub.status.idle":"2025-05-30T16:13:28.815720Z","shell.execute_reply.started":"2025-05-30T16:13:27.931689Z","shell.execute_reply":"2025-05-30T16:13:28.814897Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ktrapeznikov/biobert_v1.1_pubmed_squad_v2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"import torch\n\ndef answer_question(question, contexts, model, tokenizer, device=\"cuda\"):\n    best_answer = \"\"\n    best_score = float(\"-inf\")\n\n    for context in contexts:\n        inputs = tokenizer(\n            question, context, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n        ).to(device)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n        # Prendi le migliori posizioni inizio/fine\n        start_idx = torch.argmax(start_logits)\n        end_idx = torch.argmax(end_logits)\n\n        if start_idx <= end_idx:\n            score = start_logits[0][start_idx] + end_logits[0][end_idx]\n            answer = tokenizer.convert_tokens_to_string(\n                tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1])\n            )\n\n            if score > best_score and answer.strip():\n                best_score = score\n                best_answer = answer.strip()\n\n    return best_answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:13:38.822087Z","iopub.execute_input":"2025-05-30T16:13:38.822648Z","iopub.status.idle":"2025-05-30T16:13:38.827923Z","shell.execute_reply.started":"2025-05-30T16:13:38.822624Z","shell.execute_reply":"2025-05-30T16:13:38.827363Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"question = \"What is the most likely diagnosis for a patient with memory loss and depression?\"\ntop_chunks = retrieve_top_k(question, k=10)\nfinal_answer = answer_question(question, top_chunks, qa_model, qa_tokenizer)\n\nprint(\"ğŸ§  Final Answer:\", final_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:13:47.189598Z","iopub.execute_input":"2025-05-30T16:13:47.190259Z","iopub.status.idle":"2025-05-30T16:13:47.475296Z","shell.execute_reply.started":"2025-05-30T16:13:47.190237Z","shell.execute_reply":"2025-05-30T16:13:47.474592Z"}},"outputs":[{"name":"stdout","text":"ğŸ§  Final Answer: [CLS]\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"submission = []\n\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating answers\"):\n    qid = row[\"id\"]\n    question = row[\"question\"]\n    top_chunks = retrieve_top_k(question, k=10)\n    answer = answer_question(question, top_chunks, qa_model, qa_tokenizer)\n    submission.append((qid, answer))\n\n# Salva il file CSV finale\nsubmission_df = pd.DataFrame(submission, columns=[\"id\", \"answer\"])\nsubmission_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:14:03.404592Z","iopub.execute_input":"2025-05-30T16:14:03.405092Z","iopub.status.idle":"2025-05-30T16:18:55.300269Z","shell.execute_reply.started":"2025-05-30T16:14:03.405068Z","shell.execute_reply":"2025-05-30T16:18:55.299683Z"}},"outputs":[{"name":"stderr","text":"Generating answers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1180/1180 [04:51<00:00,  4.04it/s]\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import pandas as pd\n\nprint(\"=== Submission Data Summary ===\")\nprint(f\"Total number of answers: {len(submission_df)}\")\n\n# Basic info\nprint(\"\\nInfo:\")\nprint(submission_df.info())\n\n# Check for empty or very short answers\nsubmission_df['answer_length'] = submission_df['answer'].apply(lambda x: len(str(x).split()))\n\nprint(\"\\nAnswer length statistics:\")\nprint(submission_df['answer_length'].describe())\n\nempty_answers = submission_df[submission_df['answer_length'] <= 1]\nprint(f\"\\nNumber of empty or very short answers (<=1 token): {len(empty_answers)}\")\n\nprint(\"\\nExamples of empty or very short answers:\")\nprint(empty_answers.head(10))\n\n# Most common answers \nprint(\"\\nTop 10 most common answers:\")\nprint(submission_df['answer'].value_counts().head(10))\n\n# answer uniqueness\nunique_count = submission_df['answer'].nunique()\nprint(f\"\\nNumber of unique answers: {unique_count}\")\n\nsubmission_df[['id', 'answer', 'answer_length']].to_csv(\"submission_analysis.csv\", index=False)\nprint(\"\\nSaved submission analysis to submission_analysis.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:23:48.005095Z","iopub.execute_input":"2025-05-30T16:23:48.005353Z","iopub.status.idle":"2025-05-30T16:23:48.069714Z","shell.execute_reply.started":"2025-05-30T16:23:48.005337Z","shell.execute_reply":"2025-05-30T16:23:48.068984Z"}},"outputs":[{"name":"stdout","text":"=== Submission Data Summary ===\nTotal number of answers: 1180\n\nInfo:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1180 entries, 0 to 1179\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      1180 non-null   int64 \n 1   answer  1180 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 18.6+ KB\nNone\n\nAnswer length statistics:\ncount    1180.000000\nmean        3.796610\nstd        10.471383\nmin         1.000000\n25%         1.000000\n50%         1.000000\n75%         3.000000\nmax       220.000000\nName: answer_length, dtype: float64\n\nNumber of empty or very short answers (<=1 token): 752\n\nExamples of empty or very short answers:\n      id  answer  answer_length\n0   1702   [CLS]              1\n1   3135   [CLS]              1\n2    650   [CLS]              1\n3    741   [CLS]              1\n4   2949   [CLS]              1\n5    316   [CLS]              1\n7    386   [CLS]              1\n10  3851   [CLS]              1\n11  1119  AAUAAA              1\n13   885   [CLS]              1\n\nTop 10 most common answers:\nanswer\n[CLS]                                                  652\nunclear                                                  4\nSushi. R                                                 2\nautosomal recessive                                      2\nplatelet - derived growth factor receptor - Î±            2\nBapineuzumab                                             2\n3D physiological in vitro structures                     2\nthe efficacy of the treatment remains controversial      2\nX                                                        2\nBoolean analysis                                         2\nName: count, dtype: int64\n\nNumber of unique answers: 517\n\nSaved submission analysis to submission_analysis.csv\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"The number of placeholders here is bigger than I thought, apparenty FAISS is the best performing method, but this way we manage to answer some questions that FAISS was not able to","metadata":{}}]}